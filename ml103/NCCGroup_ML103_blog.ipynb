{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svg image](data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%20standalone%3D%22no%22%3F%3E%0A%3C%21--%20Created%20with%20Inkscape%20%28http%3A%2F%2Fwww.inkscape.org%2F%29%20--%3E%0A%0A%3Csvg%0A%20%20%20width%3D%22404%22%0A%20%20%20height%3D%22104%22%0A%20%20%20viewBox%3D%220%200%2033.24516%208.55816%22%0A%20%20%20version%3D%221.1%22%0A%20%20%20id%3D%22svg4514%22%0A%20%20%20inkscape%3Aversion%3D%221.1.2%20%280a00cf5339%2C%202022-02-04%29%22%0A%20%20%20sodipodi%3Adocname%3D%22NCC_Group_logo_adj.svg%22%0A%20%20%20xmlns%3Ainkscape%3D%22http%3A%2F%2Fwww.inkscape.org%2Fnamespaces%2Finkscape%22%0A%20%20%20xmlns%3Asodipodi%3D%22http%3A%2F%2Fsodipodi.sourceforge.net%2FDTD%2Fsodipodi-0.dtd%22%0A%20%20%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%0A%20%20%20xmlns%3Asvg%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%0A%20%20%20xmlns%3Ardf%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2F02%2F22-rdf-syntax-ns%23%22%0A%20%20%20xmlns%3Acc%3D%22http%3A%2F%2Fcreativecommons.org%2Fns%23%22%0A%20%20%20xmlns%3Adc%3D%22http%3A%2F%2Fpurl.org%2Fdc%2Felements%2F1.1%2F%22%3E%0A%20%20%3Cdefs%0A%20%20%20%20%20id%3D%22defs4508%22%3E%0A%20%20%20%20%3CclipPath%0A%20%20%20%20%20%20%20id%3D%22clipPath4085%22%0A%20%20%20%20%20%20%20clipPathUnits%3D%22userSpaceOnUse%22%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4083%22%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20473.173%2C20.811%20h%2095.881%20v%2024.544%20h%20-95.881%20z%22%20%2F%3E%0A%20%20%20%20%3C%2FclipPath%3E%0A%20%20%3C%2Fdefs%3E%0A%20%20%3Csodipodi%3Anamedview%0A%20%20%20%20%20id%3D%22base%22%0A%20%20%20%20%20pagecolor%3D%22%23ffffff%22%0A%20%20%20%20%20bordercolor%3D%22%23666666%22%0A%20%20%20%20%20borderopacity%3D%221.0%22%0A%20%20%20%20%20inkscape%3Apageopacity%3D%220.0%22%0A%20%20%20%20%20inkscape%3Apageshadow%3D%222%22%0A%20%20%20%20%20inkscape%3Azoom%3D%227.1938101%22%0A%20%20%20%20%20inkscape%3Acx%3D%2288.756861%22%0A%20%20%20%20%20inkscape%3Acy%3D%2215.638445%22%0A%20%20%20%20%20inkscape%3Adocument-units%3D%22px%22%0A%20%20%20%20%20inkscape%3Acurrent-layer%3D%22layer1%22%0A%20%20%20%20%20showgrid%3D%22false%22%0A%20%20%20%20%20fit-margin-top%3D%220%22%0A%20%20%20%20%20fit-margin-left%3D%220%22%0A%20%20%20%20%20fit-margin-right%3D%220%22%0A%20%20%20%20%20fit-margin-bottom%3D%220%22%0A%20%20%20%20%20units%3D%22px%22%0A%20%20%20%20%20inkscape%3Awindow-width%3D%222498%22%0A%20%20%20%20%20inkscape%3Awindow-height%3D%221376%22%0A%20%20%20%20%20inkscape%3Awindow-x%3D%2262%22%0A%20%20%20%20%20inkscape%3Awindow-y%3D%2227%22%0A%20%20%20%20%20inkscape%3Awindow-maximized%3D%221%22%0A%20%20%20%20%20inkscape%3Apagecheckerboard%3D%220%22%0A%20%20%20%20%20width%3D%22151.18111px%22%0A%20%20%20%20%20scale-x%3D%220.16458%22%20%2F%3E%0A%20%20%3Cmetadata%0A%20%20%20%20%20id%3D%22metadata4511%22%3E%0A%20%20%20%20%3Crdf%3ARDF%3E%0A%20%20%20%20%20%20%3Ccc%3AWork%0A%20%20%20%20%20%20%20%20%20rdf%3Aabout%3D%22%22%3E%0A%20%20%20%20%20%20%20%20%3Cdc%3Aformat%3Eimage%2Fsvg%2Bxml%3C%2Fdc%3Aformat%3E%0A%20%20%20%20%20%20%20%20%3Cdc%3Atype%0A%20%20%20%20%20%20%20%20%20%20%20rdf%3Aresource%3D%22http%3A%2F%2Fpurl.org%2Fdc%2Fdcmitype%2FStillImage%22%20%2F%3E%0A%20%20%20%20%20%20%3C%2Fcc%3AWork%3E%0A%20%20%20%20%3C%2Frdf%3ARDF%3E%0A%20%20%3C%2Fmetadata%3E%0A%20%20%3Cg%0A%20%20%20%20%20inkscape%3Alabel%3D%22Layer%201%22%0A%20%20%20%20%20inkscape%3Agroupmode%3D%22layer%22%0A%20%20%20%20%20id%3D%22layer1%22%0A%20%20%20%20%20transform%3D%22translate%2859.5848%2C-45.41591%29%22%3E%0A%20%20%20%20%3Cg%0A%20%20%20%20%20%20%20id%3D%22g4649%22%0A%20%20%20%20%20%20%20transform%3D%22translate%280.13229155%2C-0.13229107%29%22%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20-53.513353%2C49.27383%20c%200.397933%2C0%200.752828%2C0.183092%200.975078%2C0.501297%20l%200.01693%2C0.0247%200.513292%2C-0.513292%20-0.01482%2C-0.01764%20c%20-0.04057%2C-0.04868%20-0.07832%2C-0.09066%20-0.115005%2C-0.128058%20-0.381706%2C-0.388409%20-0.853723%2C-0.585259%20-1.40335%2C-0.585259%20-0.51435%2C0%20-1.008239%2C0.225073%20-1.389945%2C0.634295%20-0.349955%2C0.371475%20-0.535164%2C0.849489%20-0.535164%2C1.381478%200%2C0.51188%200.210609%2C1.036461%200.563386%2C1.402644%200.372886%2C0.393347%200.842786%2C0.592667%201.396295%2C0.592667%200.54363%2C0%201.033639%2C-0.221898%201.417814%2C-0.641703%20l%200.05962%2C-0.0695%200.01446%2C-0.01729%20-0.512939%2C-0.513292%20-0.01729%2C0.02434%20c%20-0.222603%2C0.317147%20-0.572911%2C0.49918%20-0.961672%2C0.49918%20-0.691797%2C0%20-1.213909%2C-0.552097%20-1.213909%2C-1.284111%200%2C-0.735894%200.518937%2C-1.290461%201.207206%2C-1.290461%22%0A%20%20%20%20%20%20%20%20%20style%3D%22fill%3A%239b1627%3Bfill-opacity%3A1%3Bfill-rule%3Anonzero%3Bstroke%3Anone%3Bstroke-width%3A0.352778%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4089%22%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%20%2F%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20-48.731592%2C51.224232%20c%20-0.210608%2C0.390878%20-0.598664%2C0.624064%20-1.038225%2C0.624064%20-0.692503%2C0%20-1.213908%2C-0.552097%20-1.213908%2C-1.284111%200%2C-0.735895%200.518583%2C-1.290461%201.206852%2C-1.290461%200.449792%2C0%200.842787%2C0.234244%201.050925%2C0.626886%20l%200.01588%2C0.02999%200.52705%2C-0.52705%20-0.01235%2C-0.01729%20c%20-0.0635%2C-0.08784%20-0.132291%2C-0.170392%20-0.206375%2C-0.245534%20-0.381353%2C-0.388408%20-0.853369%2C-0.585258%20-1.40335%2C-0.585258%20-0.51435%2C0%20-1.007886%2C0.225072%20-1.389239%2C0.634295%20-0.350308%2C0.371827%20-0.535516%2C0.849488%20-0.535516%2C1.38183%200%2C0.511528%200.210608%2C1.036108%200.563386%2C1.402292%200.372886%2C0.393347%200.842786%2C0.592666%201.396647%2C0.592666%200.543278%2C0%201.033286%2C-0.221544%201.417461%2C-0.641702%200.0508%2C-0.05609%200.09984%2C-0.117828%200.149225%2C-0.188384%20l%200.01199%2C-0.01729%20-0.524581%2C-0.52458%20z%22%0A%20%20%20%20%20%20%20%20%20style%3D%22fill%3A%239b1627%3Bfill-opacity%3A1%3Bfill-rule%3Anonzero%3Bstroke%3Anone%3Bstroke-width%3A0.352778%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4093%22%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%20%2F%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20-57.789161%2C48.540228%20c%20-0.956733%2C0%20-1.795639%2C0.778581%20-1.795639%2C1.673578%20v%202.252839%20h%200.775758%20l%207.06e-4%2C-2.263423%20c%200%2C-0.500944%200.547864%2C-0.889352%201.019175%2C-0.889352%200.471311%2C0%201.019175%2C0.388408%201.019175%2C0.889352%20h%200.02434%20l%20-0.02505%2C0.0092%200.0011%2C2.25425%20h%200.775758%20v%20-2.2606%20c%200%2C-0.887236%20-0.838905%2C-1.665817%20-1.795286%2C-1.665817%22%0A%20%20%20%20%20%20%20%20%20style%3D%22fill%3A%239b1627%3Bfill-opacity%3A1%3Bfill-rule%3Anonzero%3Bstroke%3Anone%3Bstroke-width%3A0.352778%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4097%22%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%20%2F%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20-39.652997%2C51.937337%20c%20-0.777169%2C0%20-1.363486%2C-0.617361%20-1.363486%2C-1.436158%200%2C-0.821973%200.583142%2C-1.442156%201.35643%2C-1.442156%200.773289%2C0%201.356784%2C0.617361%201.356784%2C1.435806%200%2C0.822325%20-0.58032%2C1.442508%20-1.349728%2C1.442508%20m%201.901119%2C-1.478492%20c%200%2C-0.499533%20-0.20073%2C-0.979311%20-0.565502%2C-1.351138%20-0.372886%2C-0.379589%20-0.83432%2C-0.572206%20-1.3716%2C-0.572206%20-0.501298%2C0%20-0.982486%2C0.220486%20-1.355725%2C0.620889%20-0.347486%2C0.368653%20-0.523523%2C0.823383%20-0.523523%2C1.352903%200%2C0.500944%200.206023%2C1.014236%200.551392%2C1.373011%200.364067%2C0.383822%200.822678%2C0.578555%201.363839%2C0.578555%200.529872%2C0%201.008592%2C-0.217311%201.383594%2C-0.627239%200.328789%2C-0.363714%200.517525%2C-0.865011%200.517525%2C-1.374775%22%0A%20%20%20%20%20%20%20%20%20style%3D%22fill%3A%239b1627%3Bfill-opacity%3A1%3Bfill-rule%3Anonzero%3Bstroke%3Anone%3Bstroke-width%3A0.352778%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4101%22%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%20%2F%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20-45.816765%2C52.000484%20c%20-0.777522%2C0%20-1.363839%2C-0.617361%20-1.363839%2C-1.435806%200%2C-0.822677%200.583495%2C-1.442861%201.356783%2C-1.442861%200.773289%2C0%201.356431%2C0.617714%201.356431%2C1.435806%200%2C0.822678%20-0.580319%2C1.442861%20-1.349375%2C1.442861%20m%200.02152%2C1.926167%20c%201.176867%2C0%201.8796%2C-0.772936%201.8796%2C-2.067278%20v%20-1.340556%20c%20-0.0011%2C-0.498122%20-0.201436%2C-0.977194%20-0.565502%2C-1.347963%20-0.372886%2C-0.379589%20-0.83432%2C-0.572206%20-1.371953%2C-0.572206%20-0.500592%2C0%20-0.982133%2C0.220486%20-1.355725%2C0.620889%20-0.347133%2C0.368653%20-0.52317%2C0.823736%20-0.52317%2C1.352903%200%2C0.500944%200.206023%2C1.014588%200.551392%2C1.373011%200.364067%2C0.383822%200.822678%2C0.578202%201.363839%2C0.578202%200.460728%2C0%200.872067%2C-0.157691%201.223786%2C-0.469547%20l%200.143228%2C-0.127%20-0.0046%2C0.191559%20c%20-0.01905%2C0.794102%20-0.575381%2C1.327502%20-1.3843%2C1.327502%20-0.465314%2C0%20-0.833614%2C-0.191205%20-1.067506%2C-0.553508%20h%20-0.573264%20c%200.277636%2C0.658636%200.884767%2C1.033992%201.684161%2C1.033992%22%0A%20%20%20%20%20%20%20%20%20style%3D%22fill%3A%239b1627%3Bfill-opacity%3A1%3Bfill-rule%3Anonzero%3Bstroke%3Anone%3Bstroke-width%3A0.352778%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4105%22%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%20%2F%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20-41.626154%2C49.152792%20v%20-0.562681%20c%20-0.444853%2C0.04269%20-0.869244%2C0.26035%20-1.202619%2C0.618067%20-0.342195%2C0.363008%20-0.519289%2C0.810683%20-0.52705%2C1.330678%20l%20-0.0011%2C0.04092%20v%201.837619%20h%200.585611%20v%20-1.873602%20c%200.0088%2C-0.72002%200.486834%2C-1.294695%201.145117%2C-1.391003%22%0A%20%20%20%20%20%20%20%20%20style%3D%22fill%3A%239b1627%3Bfill-opacity%3A1%3Bfill-rule%3Anonzero%3Bstroke%3Anone%3Bstroke-width%3A0.352778%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4109%22%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%20%2F%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20-33.819181%2C48.708927%20h%20-0.583142%20l%20-0.0035%2C2.301875%20c%20-0.04798%2C0.551744%20-0.633237%2C0.955675%20-1.159228%2C0.955675%20-0.525992%2C0%20-1.11125%2C-0.403931%20-1.159228%2C-0.955675%20h%20-0.0025%20l%20-0.0011%2C-0.08361%20v%20-2.218267%20h%20-0.583141%20v%202.218267%20c%200%2C0.866422%200.815975%2C1.619602%201.745897%2C1.619602%200.929922%2C0%201.745897%2C-0.75318%201.745897%2C-1.611488%20z%22%0A%20%20%20%20%20%20%20%20%20style%3D%22fill%3A%239b1627%3Bfill-opacity%3A1%3Bfill-rule%3Anonzero%3Bstroke%3Anone%3Bstroke-width%3A0.352778%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4113%22%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%20%2F%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20-31.426537%2C52.000484%20c%20-0.777522%2C0%20-1.363486%2C-0.617361%20-1.363486%2C-1.435806%200%2C-0.822677%200.582789%2C-1.442861%201.35643%2C-1.442861%200.773289%2C0%201.356431%2C0.617714%201.356431%2C1.435806%200%2C0.822678%20-0.579614%2C1.442861%20-1.349375%2C1.442861%20m%201.901119%2C-1.478492%20c%200%2C-0.499533%20-0.20073%2C-0.979664%20-0.56515%2C-1.351138%20-0.372886%2C-0.379589%20-0.834672%2C-0.572206%20-1.371952%2C-0.572206%20-0.500945%2C0%20-0.982486%2C0.220486%20-1.355725%2C0.620889%20-0.347486%2C0.368653%20-0.523523%2C0.823736%20-0.523523%2C1.352903%20v%203.27025%20h%200.552098%20v%20-1.884187%20l%200.138289%2C0.120298%20c%200.339019%2C0.295275%200.751416%2C0.444852%201.224844%2C0.444852%200.530225%2C0%201.008944%2C-0.216958%201.383947%2C-0.626886%200.328436%2C-0.363714%200.517172%2C-0.864658%200.517172%2C-1.374775%22%0A%20%20%20%20%20%20%20%20%20style%3D%22fill%3A%239b1627%3Bfill-opacity%3A1%3Bfill-rule%3Anonzero%3Bstroke%3Anone%3Bstroke-width%3A0.352778%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4117%22%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%20%2F%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20-26.593764%2C47.232728%20-0.0032%2C-0.05009%20c%20-0.0381%2C-0.01976%20-0.0829%2C-0.04022%20-0.137231%2C-0.06209%20-0.426155%2C-0.170745%20-0.697794%2C0.04127%20-0.731308%2C0.07338%20l%20-0.07585%2C0.07091%20c%200.264936%2C0.01305%200.480483%2C0.116416%200.601486%2C0.124883%200.117122%2C0.0078%200.235656%2C-0.03845%200.346075%2C-0.156986%22%0A%20%20%20%20%20%20%20%20%20style%3D%22fill%3A%239b1627%3Bfill-opacity%3A1%3Bfill-rule%3Anonzero%3Bstroke%3Anone%3Bstroke-width%3A0.352778%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4121%22%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%20%2F%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20-28.071585%2C47.12633%20c%20-0.07867%2C0.121003%20-0.187678%2C0.223661%20-0.31115%2C0.287867%20h%200.03739%20c%200.239889%2C0.0095%200.470606%2C-0.14605%200.607837%2C-0.358775%200.07232%2C-0.11042%200.159455%2C-0.187325%200.253294%2C-0.239184%20-0.198614%2C-0.0095%20-0.449792%2C0.09913%20-0.587375%2C0.310092%22%0A%20%20%20%20%20%20%20%20%20style%3D%22fill%3A%239b1627%3Bfill-opacity%3A1%3Bfill-rule%3Anonzero%3Bstroke%3Anone%3Bstroke-width%3A0.352778%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4125%22%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%20%2F%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20-26.857183%2C47.777523%20c%20-0.354894%2C0.01693%20-0.676275%2C-0.334433%20-1.075619%2C-0.213078%20-0.20955%2C0.0635%20-0.357364%2C0.285045%20-0.644878%2C0.372181%20-0.42545%2C0.129469%20-0.66922%2C-0.190853%20-1.081617%2C-0.06526%20-0.04657%2C0.01411%20-0.09172%2C0.03493%20-0.134761%2C0.06279%200.174978%2C0.452261%200.545394%2C0.825853%201.041047%2C0.98425%200.868539%2C0.277284%201.807281%2C-0.206727%202.084564%2C-1.075619%200.01482%2C-0.04657%200.0187%2C-0.06773%200.02963%2C-0.1143%20-0.06738%2C0.02258%20-0.134761%2C0.04516%20-0.218369%2C0.04904%22%0A%20%20%20%20%20%20%20%20%20style%3D%22fill%3A%239b1627%3Bfill-opacity%3A1%3Bfill-rule%3Anonzero%3Bstroke%3Anone%3Bstroke-width%3A0.352778%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4129%22%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%20%2F%3E%0A%20%20%20%20%20%20%3Cpath%0A%20%20%20%20%20%20%20%20%20d%3D%22m%20-29.179307%2C47.78539%20c%200.193322%2C0.04445%200.335492%2C0.0568%200.471311%2C0.01588%200.138642%2C-0.04269%200.25012%2C-0.09878%200.365125%2C-0.19685%20-0.05468%2C-0.0067%20-0.112889%2C-0.01799%20-0.177094%2C-0.03281%20-0.366536%2C-0.08431%20-0.597253%2C0.0762%20-0.649817%2C0.134761%200.06668%2C-0.09772%200.166158%2C-0.160161%200.270581%2C-0.199672%20-0.162984%2C0.0025%20-0.500239%2C0.03669%20-0.657225%2C0.266347%20l%20-0.01058%2C0.01517%20c%20-0.0064%2C-0.02258%200.14605%2C-0.34925%200.582789%2C-0.37465%200.437092%2C-0.02505%200.591961%2C-0.14605%200.826558%2C-0.60078%200.02187%2C-0.04163%200.212373%2C-0.388761%200.773289%2C-0.339373%200.136878%2C0.012%200.242006%2C0.07267%200.441325%2C-0.02434%20l%200.06632%2C-0.03881%20c%20-0.201437%2C-0.306916%20-0.492125%2C-0.531989%20-0.867128%2C-0.65158%20-0.868892%2C-0.277284%20-1.807281%2C0.206727%20-2.084211%2C1.075266%20-0.118181%2C0.369359%20-0.09807%2C0.751417%200.03034%2C1.089378%200.09737%2C-0.07514%200.310797%2C-0.208844%200.618419%2C-0.137936%22%0A%20%20%20%20%20%20%20%20%20style%3D%22fill%3A%239b1627%3Bfill-opacity%3A1%3Bfill-rule%3Anonzero%3Bstroke%3Anone%3Bstroke-width%3A0.352778%22%0A%20%20%20%20%20%20%20%20%20id%3D%22path4133%22%0A%20%20%20%20%20%20%20%20%20inkscape%3Aconnector-curvature%3D%220%22%20%2F%3E%0A%20%20%20%20%3C%2Fg%3E%0A%20%20%3C%2Fg%3E%0A%3C%2Fsvg%3E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 103: Exploring LLM Code Generation\n",
    "\n",
    "Eric Schorn      \n",
    "Technical Director      \n",
    "Cryptography Services Practice, NCC Group      \n",
    "April 15, 2023 \n",
    "\n",
    "This executable blog post is the third in a series related to machine learning and explores code generation from a 16 billion parameter large language model (LLM). After a brief look under the hood at the LLM structure and parameter allocation, we generate a variety of Python functions and make observations related to code quality and security. Similar to OpenAI's [ChatGPT](https://openai.com/blog/chatgpt), Google's [Bard](https://blog.google/technology/ai/bard-google-ai-search-updates/), Meta's [LLaMa](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) and GitHub's [Copilot](https://github.com/features/copilot), we will see some fantastic capabilities and a few intriguing misses. The results demonstrate that human expertise remains crucial, whether it be in engineering the prompt or in evaluating the generated code for suitability.\n",
    "\n",
    "The model used here was introduced in the paper titled [CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis](https://arxiv.org/abs/2203.13474) from late 2022, with [code on GitHub](https://github.com/salesforce/codegen), and is supported via the [Hugging Face](https://huggingface.co/) framework. As the paper suggests, in some respects this model's performance is comparable to that of the OpenAI Codex model which powered the GitHub Copilot product. Since this technology is still in its infancy, any misadventures should be interpreted as opportunities for further research and the need for human technical expertise, rather than as criticism.\n",
    "\n",
    "You can run everything for yourself by loading this [.ipynb file](https://github.com/nccgroup/ataraxy/blob/main/ml103/NCCGroup_ML103_blog.ipynb) into any Jupyter-based notebook system. The goal for the code below is to utilize state-of-the-art models while maximizing simplicity, understandability, and accessibility. This is consistent with the two prior posts in the blog series:\n",
    "\n",
    "* [Machine Learning 101: The Integrity of Image (Mis)Classification?](https://research.nccgroup.com/2022/12/15/machine-learning-101-the-integrity-of-image-misclassification/)\n",
    "* [Machine Learning 102: Attacking Facial Authentication with Poisoned Data](https://research.nccgroup.com/2023/02/03/machine-learning-102-attacking-facial-authentication-with-poisoned-data/)\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "The code starts here with importing standard machine learning frameworks, implementing a 'pretty printer' support function to help manage voluminous output, and then reporting key version information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.13.1.post200 with Transformers version 4.26.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torchinfo import summary\n",
    "import warnings; warnings.filterwarnings(\"ignore\")  # Pesky Pytorch artifact\n",
    "\n",
    "def pprint(text, start=0, stop=20):  # For pretty-printing with line numbers\n",
    "    lines = text.split(\"\\n\")\n",
    "    for i, line in enumerate(lines):\n",
    "        if i < start or i > stop: continue\n",
    "        print(f\"{i:>3d}: {line}\")\n",
    "\n",
    "print(\"PyTorch version {} with Transformers version {}\".format(\n",
    "    torch.__version__, transformers.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now the pretrained model can be loaded along with its matching tokenizer, and the total number of model parameters reported. This model was initially trained on 825GB of English text, then training continued on a subset of the BigQuery dataset containing multiple programming languages, and finally training concluded on a large amount of Python source code. While this Jupyter notebook does not require a GPU, the model does consume roughly 80GB of memory (with smaller but less capable models also available). The total parameter count is reported via our 'pretty printer' `pprint()` function which extracts line 419 of the model summary. The full summary is included at the very end of this post. This parameter count includes some ancillary functionality so we will only be working with the most significant digits below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419: Total params: 16,032,155,648\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-16B-mono\", pad_token_id=50256)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-16B-mono\")\n",
    "model_summary = summary(model, depth=5).__str__()\n",
    "pprint(model_summary, 419, 419)  # Extract one specific line of output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how does one 'spend' 16 billion parameters in a LLM?\n",
    "\n",
    "<br>\n",
    "\n",
    "# The transformer architecture under the hood\n",
    "\n",
    "Essentially all of the most recent and popular LLMs are based on the transformer architecture. This revolutionary architecture was launched with the paper titled [Attention Is All You Need](https://arxiv.org/abs/1706.03762) in 2017. While transformers have three broad categories (the encoder, the decoder, and the encoder-decoder), the decoder category has arguably seen the most recent publicity and is what we are using here. The basic organization is shown in the classic diagram below sourced from the detailed background available on [Wikipedia](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)). "
   ]
  },
  {
   "attachments": {
    "The-Transformer-model-architecture.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The-Transformer-model-architecture.png](attachment:The-Transformer-model-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder variant of a transformer is constrained to the components shown on the above right without any of those shown on the above left (which is the encoder). The larger block is repeated many times -- 34 in our case. As suggested by the bottom-most label, during inference the output of the model is repeatedly prepended to the input and the model rerun for each output token.\n",
    "\n",
    "The tokenizer maps an input prompt string and reference vocabulary to an input list of tokens (which are just integers). Incredibly, the transformer model simply consumes the list of tokens and performs about a (figurative) trillion multiply-accumulate operations as data moves through each layer, which finally results in a prediction for the next most likely set of tokens. The sampled output token is then appended to the input token list and the prediction process rerun again and again. Ultimately, this results the original list of tokens with the output list appended to it, which is run through a reverse tokenizer to recover the result text.\n",
    "\n",
    "\n",
    "Let's look at the construction of each the following system components while examining how 16 billion parameters are 'spent': \n",
    "\n",
    "1. Tokenizer\n",
    "2. Word embeddings\n",
    "3. Repeated blocks consisting of\n",
    "   - Self-attention mechanism\n",
    "   - Fully-connected feed-forward sublayers\n",
    "4. Linear layer with softmax output.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. Tokenizer\n",
    "\n",
    "The tokenizer is not strictly a part of the transformer model and does not consume parameters itself. Rather, it is a parsing and mapping function that is given a reference vocabulary set along with an input text string and produce a list of tokens corresponding to the words found in the input. The reference vocabulary can loosely be thought of as a Python set of words that each have a 'reference number' (which will later become an index to an embedding array). For example, the sentence \"The cat and the dog went to the beach\" may correspond to `[1, 76, 2, 1, 67, 42, 5, 1, 99]` if the word `the` maps to 1, the word `cat` maps to 76, the word `beach` maps to 99, and so forth -- note the repeated `the` becomes a repeated `1`. With a little thought, it becomes clear that there is considerable additional complexity around capitalization, verb tenses, word stems, punctuation and vocabulary size constraints which are outside the scope of this blog post.\n",
    "\n",
    "The LLM we are using here utilizes Byte-Pair Encoding tokenization as described by [Hugging Face](https://huggingface.co/course/chapter6/5). Below we see the vocabulary size reported, followed by a test prompt. This test prompt is tokenized and the resulting list of tokens shown. Each token is converted back to its vocabulary word and printed within pipes to show segmentation. Note how some tokens, such as the first two 4299 and 825 (which relate to `def`), may or may not consume their adjacent space. Rare words and nonsense are split, as are some number sequences. Finally, some arbitrary vocabulary tokens are chosen, decoded, and printed -- here we see some punctuation and word suffixes appear. All of our prompts below will be passed through this tokenizer, and it is interesting to note that this tokenizer is not Python specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size is 50257\n",
      "Test prompt: def def hullo(): test 1 2 3456 (4+5) muchago printf \n",
      "Encoded:     [4299, 825, 23644, 78, 33529, 1332, 352, 362, 513, 29228, 357, 19, 10, 20, 8, 881, 3839, 30812]\n",
      "Round trip:  |def| def| hull|o|():| test| 1| 2| 3|456| (|4|+|5|)| much|ago| printf|\n",
      "Arb vocab:   |\"|#|&|+|5|S||\f",
      "|ine|ale|\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizer vocabulary size is {tokenizer.vocab_size}\")\n",
    "test_prompt = \"def def hullo(): test 1 2 3456 (4+5) muchago printf\"\n",
    "encoded = tokenizer.encode(test_prompt)\n",
    "decoded = \"|\".join([tokenizer.decode(x) for x in encoded])\n",
    "print(f\"Test prompt: {test_prompt} \\nEncoded:     {encoded}\\nRound trip:  |{decoded}|\")\n",
    "arb_vocab = [tokenizer.decode(x) for x in [1,2,5,10,20,50,100,200,500,1000]]\n",
    "print(f'Arb vocab:   |{\"|\".join(arb_vocab)}|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2. Word embeddings\n",
    "\n",
    "Word embeddings are the initial stage of the transformer model and offer the capability of expanding the meaning of each vocabulary token. Effectively, they are a learned mapping from a single token number to a real-valued word vector. Consider an array with each row corresponding to the address of a vocabulary word and the row contents (or columns) corresponding to the learned word vector of 'embedding dimension'. We present each token to the array as a row address, the array does a lookup, and returns a word vector. This is repeated for all input tokens. The transformer has a vocabulary size of 51,200 and the model has an embedding dimension of 6,144. As 'pretty printed' below, this translates into 51,200 * 6,144 or 315M learned parameters (rounded to the nearest million)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: ===========================================================================\n",
      "  1: Layer (type:depth-idx)                             Param #\n",
      "  2: ===========================================================================\n",
      "  3: CodeGenForCausalLM                                 --\n",
      "  5:     Embedding: 2-1                              314,572,800\n"
     ]
    }
   ],
   "source": [
    "pprint(model_summary, 0, 3)  # Table headings\n",
    "pprint(model_summary, 5, 5)  # Embedding stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3. Repeated blocks\n",
    "\n",
    "The transformer architecture involves a large number of repeated blocks arranged one after another like a pipeline. Each block has two primary elements: a self-attention mechanism, and two fully-connected feed-forward layers.\n",
    "\n",
    "The self-attention mechanism involves three equal-sized matrices called `Q` for query, `K` for key and `V` for value. The matrices are effectively multiplied together to calculate the attention appropriate to each element of the subsequent layer inputs. With an embedding dimension of 6,144 and three square matrices, this requires 113M learned parameters. The results are then projected through another square matrix of size 6,144 * 6,144, resulting in another 38M learned parameters.\n",
    "\n",
    "The two fully connected layers consist of two matrices with an input and output size equal to the embedded dimension of 6,144, but with an additional and larger internal dimension. In this case it is 24,576. As a result we have 6,144 * 24,576 or 151M learned parameters. The second matrix reverses the dimensions, which requires another 151M learned parameters. \n",
    "\n",
    "This whole block is repeated 34 times in this model, resulting in 34 * (113M + 38M + 2 * 151M) or a total of 15.4B learned parameters for the repeated blocks. The few parameters reported below relating to the normalization layer are not dealt with here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: ===========================================================================\n",
      "  1: Layer (type:depth-idx)                             Param #\n",
      "  2: ===========================================================================\n",
      "  3: CodeGenForCausalLM                                 --\n",
      "  8:         CodeGenBlock: 3-1                      --\n",
      "  9:             LayerNorm: 4-1                    12,288\n",
      " 10:             CodeGenAttention: 4-2             --\n",
      " 11:                 Dropout: 5-1                 --\n",
      " 12:                 Dropout: 5-2                 --\n",
      " 13:                 Linear: 5-3                  113,246,208\n",
      " 14:                 Linear: 5-4                  37,748,736\n",
      " 15:             CodeGenMLP: 4-3                   --\n",
      " 16:                 Linear: 5-5                  151,019,520\n",
      " 17:                 Linear: 5-6                  151,001,088\n",
      " 18:                 NewGELUActivation: 5-7       --\n",
      " 19:                 Dropout: 5-8                 --\n"
     ]
    }
   ],
   "source": [
    "pprint(model_summary, 0, 3)   # Table headings\n",
    "pprint(model_summary, 8, 19)  # Block stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 4 Output layer\n",
    "\n",
    "The output layer is a relatively straightforward fully-connected feed-forward classifier. It is presented with 6,144 inputs from the last stacked block described above and returns the predicted probability of each of the 51,200 output tokens. This requires a matrix of 6,144 * 51,200 resulting in 315M parameters. The output is ultimately sampled and put into a \"reverse tokenizer\" for final text output. Note that the vocabulary size is not precisely matched between the tokenizer, input, and output -- this is an extraneous artifact for our purposes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: ===========================================================================\n",
      "  1: Layer (type:depth-idx)                             Param #\n",
      "  2: ===========================================================================\n",
      "  3: CodeGenForCausalLM                                 --\n",
      "417: Linear: 1-2                                      314,624,000\n"
     ]
    }
   ],
   "source": [
    "pprint(model_summary, 0, 3)      # Table headings\n",
    "pprint(model_summary, 417, 417)  # Output layer stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a total of 315M + 15.4B + 315M = 16B parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# LLM code generation\n",
    "\n",
    "Now we will survey code samples generated by the 16 billion parameter model described above. The helper function below will be central to this: the given prompt is tokenized, the tokens injected into the model repeatedly, and the final output tokens decoded into a returned result string. Note that beam search means the probabilities are considered over a series of 10 tokens which improves the results significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    input_ids = tokenizer(prompt.strip(\" \\n\"), return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(**input_ids, max_new_tokens=256, do_sample=False, \n",
    "                                   num_beams=10, temperature=0.75)\n",
    "    result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1. Unit conversion and programming challenges\n",
    "\n",
    "All of the code generation samples will begin with a prompt involving the target function signature and a docstring that describes the desired behavior. The `generate()` function implemented above will utilize this to produce model output which is presented through the 'pretty print' function `pprint()` (where the two numerical arguments indicate what line range to print).\n",
    "\n",
    "A simple prompt to stimulate the generation of a unit conversion helper function is supplied below. The model generates what it thinks is the most likely completion, and is allowed to continue running for a few extra lines, almost as if daydreaming about related code. It is interesting to see that the functionality is correct, and that the subsequent functions are related, correct, have appropriate names and also have type hints. As the training material has relatively minimal type hints, we drop this aspect moving forward in favor of a more specific docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def min2sec(minutes: float):\n",
      "  1:     \"\"\"Converts the input minutes into seconds.\"\"\"\n",
      "  2:     return minutes * 60\n",
      "  3: \n",
      "  4: \n",
      "  5: def sec2min(seconds: float):\n",
      "  6:     \"\"\"Converts the input seconds into minutes.\"\"\"\n",
      "  7:     return seconds / 60\n",
      "  8: \n",
      "  9: \n",
      " 10: def min2hour(minutes: float):\n",
      " 11:     \"\"\"Converts the input minutes into hours.\"\"\"\n",
      " 12:     return minutes / 60\n",
      " 13: \n",
      " 14: \n",
      " 15: def hour2min(hours: float):\n",
      " 16:     \"\"\"Converts the input hours into minutes.\"\"\"\n",
      " 17:     return hours * 60\n",
      " 18: \n"
     ]
    }
   ],
   "source": [
    "pprint(generate(\"\"\"\n",
    "def min2sec(minutes: float):\n",
    "    \\\"\"\"Converts the input minutes into seconds.\\\"\"\" \"\"\"), 0, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "A prompt to stimulate the generation of another helper function, derived from a challenge in [Project Euler](https://projecteuler.net/problem=1), is supplied below. It is interesting that the docstring contains the word \"**and**\" while the generated code includes the logical-**OR** operator on line 4. Whether the code is actually correct requires some context and human thought. If the operator should instead be logical-AND, then the loop `range()` and contents could be significantly simplified. A comprehensive (human) code review would investigate the potential differences between intent, docstring and implementation across the overall application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def sum_of_multiples(a, b, c):\n",
      "  1:     \"\"\"Finds the sum of all the multiples of a and b below c.\"\"\"\n",
      "  2:     total = 0\n",
      "  3:     for i in range(1, c):\n",
      "  4:         if i % a == 0 or i % b == 0:\n",
      "  5:             total += i\n",
      "  6:     return total\n",
      "  7: \n"
     ]
    }
   ],
   "source": [
    "pprint(generate(\"\"\"\n",
    "def sum_of_multiples(a, b, c):\n",
    "    \\\"\"\"Finds the sum of all the multiples of a and b below c.\\\"\"\" \"\"\"), 0, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "A prompt to stimulate the generation of a function to validate legal email addresses is supplied below. While the docstring is simple and complete, the task is actually very complex and somewhat open-ended as evidenced by [RFC 6530](https://www.rfc-editor.org/rfc/rfc6530), [RFC 6531](https://www.rfc-editor.org/rfc/rfc6531), [RFC 6532](https://www.rfc-editor.org/rfc/rfc6532) and [RFC 6533](https://www.rfc-editor.org/rfc/rfc6533). Despite an unusual logical inversion involving the `if` condition and return values, and even without considering internationalization, Unicode and normalization, the generated code incorrectly validates `joe@..nccgroup..helloThereThisIsATest`. Furthermore, [CVE-2021-31409](https://www.cvedetails.com/cve/CVE-2021-31409/) \"Unsafe validation RegEx in EmailValidator component\" suggests there can be problems lurking below the surface when using regular expressions to process attacker-controlled input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def validate_email(address):\n",
      "  1:     \"\"\"Validates a legal email address.\"\"\"\n",
      "  2:     if not re.match(r\"^[A-Za-z0-9\\.\\+_-]+@[A-Za-z0-9\\._-]+\\.[a-zA-Z]*$\", address):\n",
      "  3:         return False\n",
      "  4:     return True\n",
      "  5: \n"
     ]
    }
   ],
   "source": [
    "pprint(generate(\"\"\"\n",
    "def validate_email(address):\n",
    "    \\\"\"\"Validates a legal email address.\\\"\"\" \"\"\"), 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2 Specialized math\n",
    "\n",
    "Elliptic curves are an extremely specialized topic for which a very concise prompt can be supplied. A prompt to stimulate the generation of a function to calculate the group law is supplied below. While the resulting code makes some assumptions on the operand types, input validity and a supporting `double()` helper function, it works through the necessary corner cases properly and is indeed correct overall! While human expertise is required to determine whether affine, projective, Jacobian or other co-ordinate systems may be the most efficient for a given context, in this basic instance the generated code is a win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def add(p1, p2):\n",
      "  1:     \"\"\"Adds two elliptic curve points in affine coordinates.\"\"\"\n",
      "  2:     if p1 == INFINITY:\n",
      "  3:         return p2\n",
      "  4:     if p2 == INFINITY:\n",
      "  5:         return p1\n",
      "  6:     x1, y1 = p1\n",
      "  7:     x2, y2 = p2\n",
      "  8:     if x1 == x2 and y1 == y2:\n",
      "  9:         return double(p1)\n",
      " 10:     elif x1 == x2:\n",
      " 11:         return INFINITY\n",
      " 12:     s = (y2 - y1) / (x2 - x1)\n",
      " 13:     x3 = s**2 - x1 - x2\n",
      " 14:     y3 = s * (x1 - x3) - y1\n",
      " 15:     return (x3, y3)\n",
      " 16: \n"
     ]
    }
   ],
   "source": [
    "pprint(generate(\"\"\"\n",
    "def add(p1, p2):\n",
    "    \\\"\"\"Adds two elliptic curve points in affine coordinates.\\\"\"\" \"\"\"), 0, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "A prompt to stimulate the generation of a function that tests for prime numbers is supplied below. The initial cases tested on lines 2, 4 and 6 are correct. The loop `range()` endpoint on line 9 for testing factors correctly involves the square root of the operand. However, this loop is inefficient in that (for example) an input of 733 will be tested for divisibility by 3, 9, and 27 among other factors, but the latter two tests cannot succeed if the former test fails (so these latter two tests would be redundant). This issue is somewhat common, so many implementations test a longer list of primes on line 6 so that the loop on line 9 can take much larger steps. In the end, for the size of integers normally used in cryptographic applications, this sort of deterministic test that checks all potential factors would 'never complete' and thus a [probabilistic primality test](https://en.wikipedia.org/wiki/Primality_test#Probabilistic_tests) would be required. As before, this requires human expertise to evaluate the suitability of generated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def is_prime(a):\n",
      "  1:     \"\"\"Determines whether the input a is a prime number.\"\"\"\n",
      "  2:     if a < 2:\n",
      "  3:         return False\n",
      "  4:     elif a == 2:\n",
      "  5:         return True\n",
      "  6:     elif a % 2 == 0:\n",
      "  7:         return False\n",
      "  8:     else:\n",
      "  9:         for i in range(3, int(a ** 0.5) + 1, 2):\n",
      " 10:             if a % i == 0:\n",
      " 11:                 return False\n",
      " 12:         return True\n",
      " 13: \n"
     ]
    }
   ],
   "source": [
    "pprint(generate(\"\"\"\n",
    "def is_prime(a):\n",
    "    \\\"\"\"Determines whether the input a is a prime number.\\\"\"\" \"\"\"), 0, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "A prompt to stimulate the generation of a function to calculate the square root of an integer modulo N is supplied below. The resulting code continues at length but its display is cut off at line 21 for brevity. It is interesting to see input validation done here, although `assert` may be more appropriate for debug rather than production. Based on the model's generated implementation of `is_prime()` above and the docstring specifying an **odd** prime, line 7 needs to be adjusted to prevent a value for N of `2`. The actual logic implemented on line 10+ is simply a lookup table. While each result is indeed correct, this function is not usable in a real-world context and it is difficult to imagine the training that led to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def sqrt(input, N):\n",
      "  1:     \"\"\"\n",
      "  2:     Calculates the square root of the input integer modulo N, where N is an\n",
      "  3:     odd prime.\n",
      "  4:     \"\"\"\n",
      "  5:     assert isinstance(input, int)\n",
      "  6:     assert isinstance(N, int)\n",
      "  7:     assert N > 1\n",
      "  8:     assert is_prime(N)\n",
      "  9: \n",
      " 10:     if input == 0:\n",
      " 11:         return 0\n",
      " 12:     elif input == 1:\n",
      " 13:         return 1\n",
      " 14:     elif input == 4:\n",
      " 15:         return 2\n",
      " 16:     elif input == 9:\n",
      " 17:         return 3\n",
      " 18:     elif input == 16:\n",
      " 19:         return 4\n",
      " 20:     elif input == 25:\n",
      " 21:         return 5\n"
     ]
    }
   ],
   "source": [
    "pprint(generate(\"\"\"\n",
    "def sqrt(input, N):\n",
    "    \\\"\"\"\n",
    "    Calculates the square root of the input integer modulo N, where N is an\n",
    "    odd prime.\n",
    "    \\\"\"\" \"\"\"), 0, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "A similar prompt to stimulate the generation of a function to calculate the square root modulo N is supplied below, but with an additional hint involving the standard [Tonelli-Shanks algorithm](https://en.wikipedia.org/wiki/Tonelli%E2%80%93Shanks_algorithm). Oddly, our input validation now raises the correct exception variants, although the constant on line 11 should be `2` (as also noted above). However, the look-up style logic that follows is gibberish. The strong hint for the correct algorithm is completely missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def sqrt(input, N):\n",
      "  1:     \"\"\"\n",
      "  2:     Calculates the square root of the positive input integer modulo N using \n",
      "  3:     the Tonelli-Shanks algorithm, where N is an odd prime. \n",
      "  4:     \"\"\"\n",
      "  5:     if not isinstance(input, int):\n",
      "  6:         raise TypeError(\"Input must be an integer.\")\n",
      "  7:     if not isinstance(N, int):\n",
      "  8:         raise TypeError(\"N must be an odd prime integer.\")\n",
      "  9:     if input < 0:\n",
      " 10:         raise ValueError(\"Input must be a positive integer.\")\n",
      " 11:     if N <= 1:\n",
      " 12:         raise ValueError(\"N must be an odd prime integer.\")\n",
      " 13:     if not is_prime(N):\n",
      " 14:         raise ValueError(\"N must be an odd prime integer.\")\n",
      " 15:     if input == 0:\n",
      " 16:         return 0\n",
      " 17:     if input == 1:\n",
      " 18:         return 1\n",
      " 19:     if input == 2:\n",
      " 20:         return 2\n",
      " 21:     if input == 3:\n",
      " 22:         return 3\n"
     ]
    }
   ],
   "source": [
    "pprint(generate(\"\"\"\n",
    "def sqrt(input, N):\n",
    "    \\\"\"\"\n",
    "    Calculates the square root of the positive input integer modulo N using \n",
    "    the Tonelli-Shanks algorithm, where N is an odd prime. \n",
    "    \\\"\"\" \"\"\"), 0, 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3 Deserialization\n",
    "\n",
    "Below we supply a prompt to stimulate the generation of a function for integer deserialization which has traditionally been an error-prone area. There are no special cases specified except for returning the deserialized integer modulo N. The generated code does not check for the input bytearray type, and would return the wrong result when given a list of large integers. Additionally, there are a host of invisible assumptions that make this code problematic, such as the big-endian format, the potential for egregiously oversized inputs leading to a (downstream) denial of service, empty input, and the unsigned (versus signed) result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def deserialize(input, N):\n",
      "  1:     \"\"\"Deserializes a bytearray input into an integer modulo N.\"\"\"\n",
      "  2:     x = 0\n",
      "  3:     for c in input:\n",
      "  4:         x = (x << 8) | c\n",
      "  5:     return x % N\n",
      "  6: \n"
     ]
    }
   ],
   "source": [
    "pprint(generate(\"\"\"\n",
    "def deserialize(input, N):\n",
    "    \\\"\"\"Deserializes a bytearray input into an integer modulo N.\\\"\"\" \"\"\"), 0, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Below, the prompt is tightened with additional constraints such as an exact length of the input bytearray type. It is encouraging to see the resulting correct input validation as well as the use of the built in `from_bytes()` method with explicit choices for little-endian and unsigned. \n",
    "\n",
    "However, what we originally wanted when writing this blog post was a function that raised an exception when the deserialized integer was equal to or larger than N. Repeated 'engineering' of the prompt was unsuccessful at generating correct code. This sort of constraint is absolutely necessary -- see \n",
    "[Technical Advisory  Arbitrary Signature Forgery in Stark Bank ECDSA Libraries (CVE-2021-43572, CVE-2021-43570, CVE-2021-43569, CVE-2021-43568, CVE-2021-43571)](https://research.nccgroup.com/2021/11/08/technical-advisory-arbitrary-signature-forgery-in-stark-bank-ecdsa-libraries/) and table 5 in [Taming the many EdDSAs](https://eprint.iacr.org/2020/1244.pdf) relating to non-canonical encodings. So we can approach the functionality we want, but not actually generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def deserialize(bytes, N):\n",
      "  1:     \"\"\"\n",
      "  2:     Deserializes a little-endian array of exactly 16 bytes into an  \n",
      "  3:     unsigned integer modulo N. This function raises a TypeError for\n",
      "  4:     any input type other than a bytearray. This function raises a \n",
      "  5:     ValueError for an incorrectly sized input.\n",
      "  6:     \"\"\"\n",
      "  7:     if not isinstance(bytes, bytearray):\n",
      "  8:         raise TypeError(\"Input must be a bytearray\")\n",
      "  9:     if len(bytes)!= 16:\n",
      " 10:         raise ValueError(\"Input must be exactly 16 bytes\")\n",
      " 11:     return int.from_bytes(bytes, \"little\", signed=False) % N\n",
      " 12: \n"
     ]
    }
   ],
   "source": [
    "# Unable to prompt for code that raises exception for result >= N\n",
    "pprint(generate(\"\"\"\n",
    "def deserialize(bytes, N):\n",
    "    \\\"\"\"\n",
    "    Deserializes a little-endian array of exactly 16 bytes into an  \n",
    "    unsigned integer modulo N. This function raises a TypeError for\n",
    "    any input type other than a bytearray. This function raises a \n",
    "    ValueError for an incorrectly sized input.\n",
    "    \\\"\"\" \"\"\"), 0, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 4 Encryption related functionality\n",
    "\n",
    "Here encryption related functionality is considered, starting with a prompt to stimulate the generation of a key derivation function. Presumably the passphrase would be validated prior to this for minimum length, required characters, etc. However, [section 5.1.1.2 of NIST SP 800-63B](https://pages.nist.gov/800-63-3/sp800-63b.html#sec5) describes the need to also normalize Unicode strings prior to hashing to avoid interoperability problems. The generated code below specifies 100,000 iterations but this is likely not enough as [Django currently specifies 720,000](https://github.com/django/django/blob/5e9aded33f46dd78c5b6050186ba54ff9b02e986/django/contrib/auth/hashers.py#L298). Further, `sha256` is not considered memory hard per NIST guidance. Finally, the docstring specifies an output length of 16 bytes but `sha256` returns 32 bytes. As an aside, line 9 should arguably read `return salt, key` to return a better separated tuple and remove (visual) ambiguity around addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def pbkdf(passphrase):\n",
      "  1:     \"\"\"\n",
      "  2:     This key derivation function converts a passphrase into an ecryption key.\n",
      "  3:     The input passphrase is a unicode string and the output key is a bytearray\n",
      "  4:     of length 16.\n",
      "  5:     \"\"\"\n",
      "  6:     salt = bytearray(os.urandom(16))\n",
      "  7:     passphrase = passphrase.encode('utf-8')\n",
      "  8:     key = hashlib.pbkdf2_hmac('sha256', passphrase, salt, 100000)\n",
      "  9:     return salt + key\n",
      " 10: \n"
     ]
    }
   ],
   "source": [
    "pprint(generate(\"\"\"\n",
    "def pbkdf(passphrase):\n",
    "    \\\"\"\"\n",
    "    This key derivation function converts a passphrase into an ecryption key.\n",
    "    The input passphrase is a unicode string and the output key is a bytearray\n",
    "    of length 16.\n",
    "    \\\"\"\" \"\"\"), 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Here a prompt is supplied to stimulate the generation of a function that performs encryption. While it is encouraging to see AES utilized, the ECB mode is likely a [lurking disaster](https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#ECB) as \"ECB is not recommended for use in cryptographic protocols.\" As an aside, smaller models return a function where encryption is performed using a single XOR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def encrypt(data, key):\n",
      "  1:     \"\"\"\n",
      "  2:     Encrypts an input data string with a key. The key must be a bytearray\n",
      "  3:     of length 16.\n",
      "  4:     \"\"\"\n",
      "  5:     aes = AES.new(key, AES.MODE_ECB)\n",
      "  6:     return aes.encrypt(data)\n",
      "  7: \n"
     ]
    }
   ],
   "source": [
    "pprint(generate(\"\"\"\n",
    "def encrypt(data, key):\n",
    "    \\\"\"\"\n",
    "    Encrypts an input data string with a key. The key must be a bytearray\n",
    "    of length 16.\n",
    "    \\\"\"\" \"\"\"), 0, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now a more detailed prompt is supplied for decryption that uses AES in GCM mode. While the most common initialization vector (IV) length is 96 bits or 12 bytes per section 8.2 of [NIST SP 800-38d](https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-38d.pdf), the generated code below somehow connects it to the block size which is 16 bytes. Similarly,  the `cipher.verify()` operation on line 9 suggests a fix tag size of 128 bits or 16 bytes, while the specification gives a broader range. Overall, this function looks very good but is still a little less than ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def aes_decrypt(ciphertext, key):\n",
      "  1:     \"\"\"\n",
      "  2:     Decrypts the input ciphertext with a key using AES in GCM mode. The key \n",
      "  3:     must be a bytearray of length 16.\n",
      "  4:     \"\"\"\n",
      "  5:     iv = ciphertext[:AES.block_size]\n",
      "  6:     cipher = AES.new(key, AES.MODE_GCM, iv)\n",
      "  7:     plaintext = cipher.decrypt(ciphertext[AES.block_size:])\n",
      "  8:     try:\n",
      "  9:         cipher.verify(plaintext[-16:])\n",
      " 10:         return plaintext[:-16]\n",
      " 11:     except ValueError:\n",
      " 12:         raise ValueError(\"Invalid key or ciphertext!\")\n",
      " 13: \n"
     ]
    }
   ],
   "source": [
    "pprint(generate(\"\"\"\n",
    "def aes_decrypt(ciphertext, key):\n",
    "    \\\"\"\"\n",
    "    Decrypts the input ciphertext with a key using AES in GCM mode. The key \n",
    "    must be a bytearray of length 16.\n",
    "    \\\"\"\" \"\"\"), 0, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Code with unit tests\n",
    "\n",
    "If increasing the productivity of lesser-skilled programmers has become concerning, perhaps automating test case generation would be a better initial step. Here a prompt is supplied to stimulate the generation of a function that calculates the greatest common denominator of its two integer inputs -- a common programming challenge. The additional validation involving `0` is arguably correct, and this is quite nice code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: def gcd(a, b):\n",
      "  1:     \"\"\"\n",
      "  2:     Finds the greatest common denominator of two positive non-zero integers \n",
      "  3:     a and b. This function raises a TypeError if both inputs are not integers.\n",
      "  4:     \"\"\"\n",
      "  5:     if not isinstance(a, int) or not isinstance(b, int):\n",
      "  6:         raise TypeError\n",
      "  7:     if a < 0 or b < 0:\n",
      "  8:         raise ValueError\n",
      "  9:     if a == 0 and b == 0:\n",
      " 10:         raise ValueError\n",
      " 11:     if a == 0:\n",
      " 12:         return b\n",
      " 13:     if b == 0:\n",
      " 14:         return a\n",
      " 15:     if a < b:\n",
      " 16:         a, b = b, a\n",
      " 17:     while b!= 0:\n",
      " 18:         a, b = b, a % b\n",
      " 19:     return a\n",
      " 20: \n"
     ]
    }
   ],
   "source": [
    "# def greatest_common_denominator(a, b):\n",
    "pprint(generate(\"\"\"\n",
    "def gcd(a, b):\n",
    "    \\\"\"\"\n",
    "    Finds the greatest common denominator of two positive non-zero integers \n",
    "    a and b. This function raises a TypeError if both inputs are not integers.\n",
    "    \\\"\"\" \"\"\"), 0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The prompt below is freeform departure from the preceding instances, and simply asks for test cases to be generated. This gives another very nice result, albeit with uninspiring choice of data. Clearly, a human would need to adapt the test vector values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: Write a unit test for a function that calculates the greatest common \n",
      "  1: denominator of two positive integers.\n",
      "  2: \"\"\"\n",
      "  3: \n",
      "  4: import unittest\n",
      "  5: from gcd import gcd\n",
      "  6: \n",
      "  7: \n",
      "  8: class TestGcd(unittest.TestCase):\n",
      "  9:     def test_gcd_of_1_and_1(self):\n",
      " 10:         self.assertEqual(1, gcd(1, 1))\n",
      " 11: \n",
      " 12:     def test_gcd_of_2_and_2(self):\n",
      " 13:         self.assertEqual(2, gcd(2, 2))\n",
      " 14: \n",
      " 15:     def test_gcd_of_3_and_3(self):\n",
      " 16:         self.assertEqual(3, gcd(3, 3))\n",
      " 17: \n",
      " 18:     def test_gcd_of_4_and_4(self):\n",
      " 19:         self.assertEqual(4, gcd(4, 4))\n",
      " 20: \n",
      " 21:     def test_gcd_of_5_and_5(self):\n",
      " 22:         self.assertEqual(5, gcd(5, 5))\n",
      " 23: \n"
     ]
    }
   ],
   "source": [
    "pprint(generate(\"\"\"\n",
    "Write a unit test for a function that calculates the greatest common \n",
    "denominator of two positive integers. \"\"\"), 0, 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "We have now looked under the hood of an LLM to understand the basic architecture, where and how 16 billion parameters are spent, and the results of asking the model to generate a variety of code fragments. At first impression, the capabilities are absolutely stunning.\n",
    "\n",
    "However, as the above examples have shown, human expertise is pivotal in correctly specifying the LLM prompt as well as evaluating the generated code output. The higher level application context typically remains unexpressed and unexamined by the model. The natural tendency for time-pressured developers to judge \"that looks about right\" along with commercial pressures to \"ship it when it seems to work\" will best-case result in a house of cards deployed into production. In the worst-case, obvious CVEs are generated.\n",
    "\n",
    "At this point, code generation by LLMs appears to change the landscape for human expertise and deep thought, but only increases rather than decreases its importance. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The author would like to thank Aleksandar Kircanski and Eli Sohl for their human expertise and detailed review. All issues remain with the author. The full model summary is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: ===========================================================================\n",
      "  1: Layer (type:depth-idx)                             Param #\n",
      "  2: ===========================================================================\n",
      "  3: CodeGenForCausalLM                                 --\n",
      "  4: CodeGenModel: 1-1                                --\n",
      "  5:     Embedding: 2-1                              314,572,800\n",
      "  6:     Dropout: 2-2                                --\n",
      "  7:     ModuleList: 2-3                             --\n",
      "  8:         CodeGenBlock: 3-1                      --\n",
      "  9:             LayerNorm: 4-1                    12,288\n",
      " 10:             CodeGenAttention: 4-2             --\n",
      " 11:                 Dropout: 5-1                 --\n",
      " 12:                 Dropout: 5-2                 --\n",
      " 13:                 Linear: 5-3                  113,246,208\n",
      " 14:                 Linear: 5-4                  37,748,736\n",
      " 15:             CodeGenMLP: 4-3                   --\n",
      " 16:                 Linear: 5-5                  151,019,520\n",
      " 17:                 Linear: 5-6                  151,001,088\n",
      " 18:                 NewGELUActivation: 5-7       --\n",
      " 19:                 Dropout: 5-8                 --\n",
      " 20:         CodeGenBlock: 3-2                      --\n",
      " 21:             LayerNorm: 4-4                    12,288\n",
      " 22:             CodeGenAttention: 4-5             --\n",
      " 23:                 Dropout: 5-9                 --\n",
      " 24:                 Dropout: 5-10                --\n",
      " 25:                 Linear: 5-11                 113,246,208\n",
      " 26:                 Linear: 5-12                 37,748,736\n",
      " 27:             CodeGenMLP: 4-6                   --\n",
      " 28:                 Linear: 5-13                 151,019,520\n",
      " 29:                 Linear: 5-14                 151,001,088\n",
      " 30:                 NewGELUActivation: 5-15      --\n",
      " 31:                 Dropout: 5-16                --\n",
      " 32:         CodeGenBlock: 3-3                      --\n",
      " 33:             LayerNorm: 4-7                    12,288\n",
      " 34:             CodeGenAttention: 4-8             --\n",
      " 35:                 Dropout: 5-17                --\n",
      " 36:                 Dropout: 5-18                --\n",
      " 37:                 Linear: 5-19                 113,246,208\n",
      " 38:                 Linear: 5-20                 37,748,736\n",
      " 39:             CodeGenMLP: 4-9                   --\n",
      " 40:                 Linear: 5-21                 151,019,520\n",
      " 41:                 Linear: 5-22                 151,001,088\n",
      " 42:                 NewGELUActivation: 5-23      --\n",
      " 43:                 Dropout: 5-24                --\n",
      " 44:         CodeGenBlock: 3-4                      --\n",
      " 45:             LayerNorm: 4-10                   12,288\n",
      " 46:             CodeGenAttention: 4-11            --\n",
      " 47:                 Dropout: 5-25                --\n",
      " 48:                 Dropout: 5-26                --\n",
      " 49:                 Linear: 5-27                 113,246,208\n",
      " 50:                 Linear: 5-28                 37,748,736\n",
      " 51:             CodeGenMLP: 4-12                  --\n",
      " 52:                 Linear: 5-29                 151,019,520\n",
      " 53:                 Linear: 5-30                 151,001,088\n",
      " 54:                 NewGELUActivation: 5-31      --\n",
      " 55:                 Dropout: 5-32                --\n",
      " 56:         CodeGenBlock: 3-5                      --\n",
      " 57:             LayerNorm: 4-13                   12,288\n",
      " 58:             CodeGenAttention: 4-14            --\n",
      " 59:                 Dropout: 5-33                --\n",
      " 60:                 Dropout: 5-34                --\n",
      " 61:                 Linear: 5-35                 113,246,208\n",
      " 62:                 Linear: 5-36                 37,748,736\n",
      " 63:             CodeGenMLP: 4-15                  --\n",
      " 64:                 Linear: 5-37                 151,019,520\n",
      " 65:                 Linear: 5-38                 151,001,088\n",
      " 66:                 NewGELUActivation: 5-39      --\n",
      " 67:                 Dropout: 5-40                --\n",
      " 68:         CodeGenBlock: 3-6                      --\n",
      " 69:             LayerNorm: 4-16                   12,288\n",
      " 70:             CodeGenAttention: 4-17            --\n",
      " 71:                 Dropout: 5-41                --\n",
      " 72:                 Dropout: 5-42                --\n",
      " 73:                 Linear: 5-43                 113,246,208\n",
      " 74:                 Linear: 5-44                 37,748,736\n",
      " 75:             CodeGenMLP: 4-18                  --\n",
      " 76:                 Linear: 5-45                 151,019,520\n",
      " 77:                 Linear: 5-46                 151,001,088\n",
      " 78:                 NewGELUActivation: 5-47      --\n",
      " 79:                 Dropout: 5-48                --\n",
      " 80:         CodeGenBlock: 3-7                      --\n",
      " 81:             LayerNorm: 4-19                   12,288\n",
      " 82:             CodeGenAttention: 4-20            --\n",
      " 83:                 Dropout: 5-49                --\n",
      " 84:                 Dropout: 5-50                --\n",
      " 85:                 Linear: 5-51                 113,246,208\n",
      " 86:                 Linear: 5-52                 37,748,736\n",
      " 87:             CodeGenMLP: 4-21                  --\n",
      " 88:                 Linear: 5-53                 151,019,520\n",
      " 89:                 Linear: 5-54                 151,001,088\n",
      " 90:                 NewGELUActivation: 5-55      --\n",
      " 91:                 Dropout: 5-56                --\n",
      " 92:         CodeGenBlock: 3-8                      --\n",
      " 93:             LayerNorm: 4-22                   12,288\n",
      " 94:             CodeGenAttention: 4-23            --\n",
      " 95:                 Dropout: 5-57                --\n",
      " 96:                 Dropout: 5-58                --\n",
      " 97:                 Linear: 5-59                 113,246,208\n",
      " 98:                 Linear: 5-60                 37,748,736\n",
      " 99:             CodeGenMLP: 4-24                  --\n",
      "100:                 Linear: 5-61                 151,019,520\n",
      "101:                 Linear: 5-62                 151,001,088\n",
      "102:                 NewGELUActivation: 5-63      --\n",
      "103:                 Dropout: 5-64                --\n",
      "104:         CodeGenBlock: 3-9                      --\n",
      "105:             LayerNorm: 4-25                   12,288\n",
      "106:             CodeGenAttention: 4-26            --\n",
      "107:                 Dropout: 5-65                --\n",
      "108:                 Dropout: 5-66                --\n",
      "109:                 Linear: 5-67                 113,246,208\n",
      "110:                 Linear: 5-68                 37,748,736\n",
      "111:             CodeGenMLP: 4-27                  --\n",
      "112:                 Linear: 5-69                 151,019,520\n",
      "113:                 Linear: 5-70                 151,001,088\n",
      "114:                 NewGELUActivation: 5-71      --\n",
      "115:                 Dropout: 5-72                --\n",
      "116:         CodeGenBlock: 3-10                     --\n",
      "117:             LayerNorm: 4-28                   12,288\n",
      "118:             CodeGenAttention: 4-29            --\n",
      "119:                 Dropout: 5-73                --\n",
      "120:                 Dropout: 5-74                --\n",
      "121:                 Linear: 5-75                 113,246,208\n",
      "122:                 Linear: 5-76                 37,748,736\n",
      "123:             CodeGenMLP: 4-30                  --\n",
      "124:                 Linear: 5-77                 151,019,520\n",
      "125:                 Linear: 5-78                 151,001,088\n",
      "126:                 NewGELUActivation: 5-79      --\n",
      "127:                 Dropout: 5-80                --\n",
      "128:         CodeGenBlock: 3-11                     --\n",
      "129:             LayerNorm: 4-31                   12,288\n",
      "130:             CodeGenAttention: 4-32            --\n",
      "131:                 Dropout: 5-81                --\n",
      "132:                 Dropout: 5-82                --\n",
      "133:                 Linear: 5-83                 113,246,208\n",
      "134:                 Linear: 5-84                 37,748,736\n",
      "135:             CodeGenMLP: 4-33                  --\n",
      "136:                 Linear: 5-85                 151,019,520\n",
      "137:                 Linear: 5-86                 151,001,088\n",
      "138:                 NewGELUActivation: 5-87      --\n",
      "139:                 Dropout: 5-88                --\n",
      "140:         CodeGenBlock: 3-12                     --\n",
      "141:             LayerNorm: 4-34                   12,288\n",
      "142:             CodeGenAttention: 4-35            --\n",
      "143:                 Dropout: 5-89                --\n",
      "144:                 Dropout: 5-90                --\n",
      "145:                 Linear: 5-91                 113,246,208\n",
      "146:                 Linear: 5-92                 37,748,736\n",
      "147:             CodeGenMLP: 4-36                  --\n",
      "148:                 Linear: 5-93                 151,019,520\n",
      "149:                 Linear: 5-94                 151,001,088\n",
      "150:                 NewGELUActivation: 5-95      --\n",
      "151:                 Dropout: 5-96                --\n",
      "152:         CodeGenBlock: 3-13                     --\n",
      "153:             LayerNorm: 4-37                   12,288\n",
      "154:             CodeGenAttention: 4-38            --\n",
      "155:                 Dropout: 5-97                --\n",
      "156:                 Dropout: 5-98                --\n",
      "157:                 Linear: 5-99                 113,246,208\n",
      "158:                 Linear: 5-100                37,748,736\n",
      "159:             CodeGenMLP: 4-39                  --\n",
      "160:                 Linear: 5-101                151,019,520\n",
      "161:                 Linear: 5-102                151,001,088\n",
      "162:                 NewGELUActivation: 5-103     --\n",
      "163:                 Dropout: 5-104               --\n",
      "164:         CodeGenBlock: 3-14                     --\n",
      "165:             LayerNorm: 4-40                   12,288\n",
      "166:             CodeGenAttention: 4-41            --\n",
      "167:                 Dropout: 5-105               --\n",
      "168:                 Dropout: 5-106               --\n",
      "169:                 Linear: 5-107                113,246,208\n",
      "170:                 Linear: 5-108                37,748,736\n",
      "171:             CodeGenMLP: 4-42                  --\n",
      "172:                 Linear: 5-109                151,019,520\n",
      "173:                 Linear: 5-110                151,001,088\n",
      "174:                 NewGELUActivation: 5-111     --\n",
      "175:                 Dropout: 5-112               --\n",
      "176:         CodeGenBlock: 3-15                     --\n",
      "177:             LayerNorm: 4-43                   12,288\n",
      "178:             CodeGenAttention: 4-44            --\n",
      "179:                 Dropout: 5-113               --\n",
      "180:                 Dropout: 5-114               --\n",
      "181:                 Linear: 5-115                113,246,208\n",
      "182:                 Linear: 5-116                37,748,736\n",
      "183:             CodeGenMLP: 4-45                  --\n",
      "184:                 Linear: 5-117                151,019,520\n",
      "185:                 Linear: 5-118                151,001,088\n",
      "186:                 NewGELUActivation: 5-119     --\n",
      "187:                 Dropout: 5-120               --\n",
      "188:         CodeGenBlock: 3-16                     --\n",
      "189:             LayerNorm: 4-46                   12,288\n",
      "190:             CodeGenAttention: 4-47            --\n",
      "191:                 Dropout: 5-121               --\n",
      "192:                 Dropout: 5-122               --\n",
      "193:                 Linear: 5-123                113,246,208\n",
      "194:                 Linear: 5-124                37,748,736\n",
      "195:             CodeGenMLP: 4-48                  --\n",
      "196:                 Linear: 5-125                151,019,520\n",
      "197:                 Linear: 5-126                151,001,088\n",
      "198:                 NewGELUActivation: 5-127     --\n",
      "199:                 Dropout: 5-128               --\n",
      "200:         CodeGenBlock: 3-17                     --\n",
      "201:             LayerNorm: 4-49                   12,288\n",
      "202:             CodeGenAttention: 4-50            --\n",
      "203:                 Dropout: 5-129               --\n",
      "204:                 Dropout: 5-130               --\n",
      "205:                 Linear: 5-131                113,246,208\n",
      "206:                 Linear: 5-132                37,748,736\n",
      "207:             CodeGenMLP: 4-51                  --\n",
      "208:                 Linear: 5-133                151,019,520\n",
      "209:                 Linear: 5-134                151,001,088\n",
      "210:                 NewGELUActivation: 5-135     --\n",
      "211:                 Dropout: 5-136               --\n",
      "212:         CodeGenBlock: 3-18                     --\n",
      "213:             LayerNorm: 4-52                   12,288\n",
      "214:             CodeGenAttention: 4-53            --\n",
      "215:                 Dropout: 5-137               --\n",
      "216:                 Dropout: 5-138               --\n",
      "217:                 Linear: 5-139                113,246,208\n",
      "218:                 Linear: 5-140                37,748,736\n",
      "219:             CodeGenMLP: 4-54                  --\n",
      "220:                 Linear: 5-141                151,019,520\n",
      "221:                 Linear: 5-142                151,001,088\n",
      "222:                 NewGELUActivation: 5-143     --\n",
      "223:                 Dropout: 5-144               --\n",
      "224:         CodeGenBlock: 3-19                     --\n",
      "225:             LayerNorm: 4-55                   12,288\n",
      "226:             CodeGenAttention: 4-56            --\n",
      "227:                 Dropout: 5-145               --\n",
      "228:                 Dropout: 5-146               --\n",
      "229:                 Linear: 5-147                113,246,208\n",
      "230:                 Linear: 5-148                37,748,736\n",
      "231:             CodeGenMLP: 4-57                  --\n",
      "232:                 Linear: 5-149                151,019,520\n",
      "233:                 Linear: 5-150                151,001,088\n",
      "234:                 NewGELUActivation: 5-151     --\n",
      "235:                 Dropout: 5-152               --\n",
      "236:         CodeGenBlock: 3-20                     --\n",
      "237:             LayerNorm: 4-58                   12,288\n",
      "238:             CodeGenAttention: 4-59            --\n",
      "239:                 Dropout: 5-153               --\n",
      "240:                 Dropout: 5-154               --\n",
      "241:                 Linear: 5-155                113,246,208\n",
      "242:                 Linear: 5-156                37,748,736\n",
      "243:             CodeGenMLP: 4-60                  --\n",
      "244:                 Linear: 5-157                151,019,520\n",
      "245:                 Linear: 5-158                151,001,088\n",
      "246:                 NewGELUActivation: 5-159     --\n",
      "247:                 Dropout: 5-160               --\n",
      "248:         CodeGenBlock: 3-21                     --\n",
      "249:             LayerNorm: 4-61                   12,288\n",
      "250:             CodeGenAttention: 4-62            --\n",
      "251:                 Dropout: 5-161               --\n",
      "252:                 Dropout: 5-162               --\n",
      "253:                 Linear: 5-163                113,246,208\n",
      "254:                 Linear: 5-164                37,748,736\n",
      "255:             CodeGenMLP: 4-63                  --\n",
      "256:                 Linear: 5-165                151,019,520\n",
      "257:                 Linear: 5-166                151,001,088\n",
      "258:                 NewGELUActivation: 5-167     --\n",
      "259:                 Dropout: 5-168               --\n",
      "260:         CodeGenBlock: 3-22                     --\n",
      "261:             LayerNorm: 4-64                   12,288\n",
      "262:             CodeGenAttention: 4-65            --\n",
      "263:                 Dropout: 5-169               --\n",
      "264:                 Dropout: 5-170               --\n",
      "265:                 Linear: 5-171                113,246,208\n",
      "266:                 Linear: 5-172                37,748,736\n",
      "267:             CodeGenMLP: 4-66                  --\n",
      "268:                 Linear: 5-173                151,019,520\n",
      "269:                 Linear: 5-174                151,001,088\n",
      "270:                 NewGELUActivation: 5-175     --\n",
      "271:                 Dropout: 5-176               --\n",
      "272:         CodeGenBlock: 3-23                     --\n",
      "273:             LayerNorm: 4-67                   12,288\n",
      "274:             CodeGenAttention: 4-68            --\n",
      "275:                 Dropout: 5-177               --\n",
      "276:                 Dropout: 5-178               --\n",
      "277:                 Linear: 5-179                113,246,208\n",
      "278:                 Linear: 5-180                37,748,736\n",
      "279:             CodeGenMLP: 4-69                  --\n",
      "280:                 Linear: 5-181                151,019,520\n",
      "281:                 Linear: 5-182                151,001,088\n",
      "282:                 NewGELUActivation: 5-183     --\n",
      "283:                 Dropout: 5-184               --\n",
      "284:         CodeGenBlock: 3-24                     --\n",
      "285:             LayerNorm: 4-70                   12,288\n",
      "286:             CodeGenAttention: 4-71            --\n",
      "287:                 Dropout: 5-185               --\n",
      "288:                 Dropout: 5-186               --\n",
      "289:                 Linear: 5-187                113,246,208\n",
      "290:                 Linear: 5-188                37,748,736\n",
      "291:             CodeGenMLP: 4-72                  --\n",
      "292:                 Linear: 5-189                151,019,520\n",
      "293:                 Linear: 5-190                151,001,088\n",
      "294:                 NewGELUActivation: 5-191     --\n",
      "295:                 Dropout: 5-192               --\n",
      "296:         CodeGenBlock: 3-25                     --\n",
      "297:             LayerNorm: 4-73                   12,288\n",
      "298:             CodeGenAttention: 4-74            --\n",
      "299:                 Dropout: 5-193               --\n",
      "300:                 Dropout: 5-194               --\n",
      "301:                 Linear: 5-195                113,246,208\n",
      "302:                 Linear: 5-196                37,748,736\n",
      "303:             CodeGenMLP: 4-75                  --\n",
      "304:                 Linear: 5-197                151,019,520\n",
      "305:                 Linear: 5-198                151,001,088\n",
      "306:                 NewGELUActivation: 5-199     --\n",
      "307:                 Dropout: 5-200               --\n",
      "308:         CodeGenBlock: 3-26                     --\n",
      "309:             LayerNorm: 4-76                   12,288\n",
      "310:             CodeGenAttention: 4-77            --\n",
      "311:                 Dropout: 5-201               --\n",
      "312:                 Dropout: 5-202               --\n",
      "313:                 Linear: 5-203                113,246,208\n",
      "314:                 Linear: 5-204                37,748,736\n",
      "315:             CodeGenMLP: 4-78                  --\n",
      "316:                 Linear: 5-205                151,019,520\n",
      "317:                 Linear: 5-206                151,001,088\n",
      "318:                 NewGELUActivation: 5-207     --\n",
      "319:                 Dropout: 5-208               --\n",
      "320:         CodeGenBlock: 3-27                     --\n",
      "321:             LayerNorm: 4-79                   12,288\n",
      "322:             CodeGenAttention: 4-80            --\n",
      "323:                 Dropout: 5-209               --\n",
      "324:                 Dropout: 5-210               --\n",
      "325:                 Linear: 5-211                113,246,208\n",
      "326:                 Linear: 5-212                37,748,736\n",
      "327:             CodeGenMLP: 4-81                  --\n",
      "328:                 Linear: 5-213                151,019,520\n",
      "329:                 Linear: 5-214                151,001,088\n",
      "330:                 NewGELUActivation: 5-215     --\n",
      "331:                 Dropout: 5-216               --\n",
      "332:         CodeGenBlock: 3-28                     --\n",
      "333:             LayerNorm: 4-82                   12,288\n",
      "334:             CodeGenAttention: 4-83            --\n",
      "335:                 Dropout: 5-217               --\n",
      "336:                 Dropout: 5-218               --\n",
      "337:                 Linear: 5-219                113,246,208\n",
      "338:                 Linear: 5-220                37,748,736\n",
      "339:             CodeGenMLP: 4-84                  --\n",
      "340:                 Linear: 5-221                151,019,520\n",
      "341:                 Linear: 5-222                151,001,088\n",
      "342:                 NewGELUActivation: 5-223     --\n",
      "343:                 Dropout: 5-224               --\n",
      "344:         CodeGenBlock: 3-29                     --\n",
      "345:             LayerNorm: 4-85                   12,288\n",
      "346:             CodeGenAttention: 4-86            --\n",
      "347:                 Dropout: 5-225               --\n",
      "348:                 Dropout: 5-226               --\n",
      "349:                 Linear: 5-227                113,246,208\n",
      "350:                 Linear: 5-228                37,748,736\n",
      "351:             CodeGenMLP: 4-87                  --\n",
      "352:                 Linear: 5-229                151,019,520\n",
      "353:                 Linear: 5-230                151,001,088\n",
      "354:                 NewGELUActivation: 5-231     --\n",
      "355:                 Dropout: 5-232               --\n",
      "356:         CodeGenBlock: 3-30                     --\n",
      "357:             LayerNorm: 4-88                   12,288\n",
      "358:             CodeGenAttention: 4-89            --\n",
      "359:                 Dropout: 5-233               --\n",
      "360:                 Dropout: 5-234               --\n",
      "361:                 Linear: 5-235                113,246,208\n",
      "362:                 Linear: 5-236                37,748,736\n",
      "363:             CodeGenMLP: 4-90                  --\n",
      "364:                 Linear: 5-237                151,019,520\n",
      "365:                 Linear: 5-238                151,001,088\n",
      "366:                 NewGELUActivation: 5-239     --\n",
      "367:                 Dropout: 5-240               --\n",
      "368:         CodeGenBlock: 3-31                     --\n",
      "369:             LayerNorm: 4-91                   12,288\n",
      "370:             CodeGenAttention: 4-92            --\n",
      "371:                 Dropout: 5-241               --\n",
      "372:                 Dropout: 5-242               --\n",
      "373:                 Linear: 5-243                113,246,208\n",
      "374:                 Linear: 5-244                37,748,736\n",
      "375:             CodeGenMLP: 4-93                  --\n",
      "376:                 Linear: 5-245                151,019,520\n",
      "377:                 Linear: 5-246                151,001,088\n",
      "378:                 NewGELUActivation: 5-247     --\n",
      "379:                 Dropout: 5-248               --\n",
      "380:         CodeGenBlock: 3-32                     --\n",
      "381:             LayerNorm: 4-94                   12,288\n",
      "382:             CodeGenAttention: 4-95            --\n",
      "383:                 Dropout: 5-249               --\n",
      "384:                 Dropout: 5-250               --\n",
      "385:                 Linear: 5-251                113,246,208\n",
      "386:                 Linear: 5-252                37,748,736\n",
      "387:             CodeGenMLP: 4-96                  --\n",
      "388:                 Linear: 5-253                151,019,520\n",
      "389:                 Linear: 5-254                151,001,088\n",
      "390:                 NewGELUActivation: 5-255     --\n",
      "391:                 Dropout: 5-256               --\n",
      "392:         CodeGenBlock: 3-33                     --\n",
      "393:             LayerNorm: 4-97                   12,288\n",
      "394:             CodeGenAttention: 4-98            --\n",
      "395:                 Dropout: 5-257               --\n",
      "396:                 Dropout: 5-258               --\n",
      "397:                 Linear: 5-259                113,246,208\n",
      "398:                 Linear: 5-260                37,748,736\n",
      "399:             CodeGenMLP: 4-99                  --\n",
      "400:                 Linear: 5-261                151,019,520\n",
      "401:                 Linear: 5-262                151,001,088\n",
      "402:                 NewGELUActivation: 5-263     --\n",
      "403:                 Dropout: 5-264               --\n",
      "404:         CodeGenBlock: 3-34                     --\n",
      "405:             LayerNorm: 4-100                  12,288\n",
      "406:             CodeGenAttention: 4-101           --\n",
      "407:                 Dropout: 5-265               --\n",
      "408:                 Dropout: 5-266               --\n",
      "409:                 Linear: 5-267                113,246,208\n",
      "410:                 Linear: 5-268                37,748,736\n",
      "411:             CodeGenMLP: 4-102                 --\n",
      "412:                 Linear: 5-269                151,019,520\n",
      "413:                 Linear: 5-270                151,001,088\n",
      "414:                 NewGELUActivation: 5-271     --\n",
      "415:                 Dropout: 5-272               --\n",
      "416:     LayerNorm: 2-4                              12,288\n",
      "417: Linear: 1-2                                      314,624,000\n",
      "418: ===========================================================================\n",
      "419: Total params: 16,032,155,648\n",
      "420: Trainable params: 16,032,155,648\n",
      "421: Non-trainable params: 0\n",
      "422: ===========================================================================\n"
     ]
    }
   ],
   "source": [
    "pprint(model_summary, 0, 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
